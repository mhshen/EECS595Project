{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mh/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all helpfulness/harmless subsets (share the same schema)\n",
    "raw_datasets = load_dataset(\"Anthropic/hh-rlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 160800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected'],\n",
       "        num_rows: 8552\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"reward_modeling_anthropic_hh\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[31373,    11,   314,   716,   534,  2988,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"hello, I am your father.\", return_tensors=\"pt\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2241]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "        new_examples = {\n",
    "            \"input_ids_chosen\": [],\n",
    "            \"attention_mask_chosen\": [],\n",
    "            \"input_ids_rejected\": [],\n",
    "            \"attention_mask_rejected\": [],\n",
    "        }\n",
    "        for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "            tokenized_chosen = tokenizer(chosen)\n",
    "            tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "            new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "            new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "            new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "        return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "raw_datasets = raw_datasets.filter(\n",
    "        lambda x: len(x[\"input_ids_chosen\"]) <= max_length\n",
    "        and len(x[\"input_ids_rejected\"]) <= max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=torch.tensor([raw_datasets[\"test\"][\"input_ids_chosen\"][0]]),\n",
    "        attention_mask=torch.tensor([raw_datasets[\"test\"][\"attention_mask_chosen\"][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2263]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95149/1976393481.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"f1\")\n",
      "/home/mh/miniconda3/envs/nlp/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8da37b7968e48ec94d5b2e5aeee8b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate f1 score on the test set\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits > 0\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8489/8489 [19:07<00:00,  7.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# make predictions on test set using model\n",
    "prediction_results = np.zeros((len(raw_datasets[\"test\"]),))\n",
    "for i, sample in enumerate(tqdm(raw_datasets[\"test\"])):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=torch.tensor([sample[\"input_ids_chosen\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_chosen\"]]))\n",
    "        sample[\"logits_chosen\"] = output.logits\n",
    "\n",
    "        output = model(input_ids=torch.tensor([sample[\"input_ids_rejected\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_rejected\"]]))\n",
    "        sample[\"logits_rejected\"] = output.logits\n",
    "        \n",
    "        prediction_results[i] = float(sample[\"logits_chosen\"] > sample[\"logits_rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5763929791494876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prediction_results) / len(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.731280824988791}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate f1 score\n",
    "compute_metrics((prediction_results, np.ones((len(prediction_results),))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd0e0e025ce458e9694e577d3c27448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ea256078a9411892668c04dc7ca6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b472c5ac9034dfca22e992defd80142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shp_datasets = load_dataset(\"stanfordnlp/shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4837bdb46141abb98ac972dea27028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/348718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c97c411cf444c408a1aec0b7ffb4eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/18436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd588ac6be9247ad83924f5c80e88fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/18409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3913f1db8b7043b9a0d264d77fbe7c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/348718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1169 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1152 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1703 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1792 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2409 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1571 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1778 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b48b6b28a7c45048bc047bfe36d54de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/18436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1387 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2239 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1945 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d1530884cd4f7fa49242f763d10c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/18409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1393 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1178 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1945 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1231 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1488 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f590de098bf4485a8deba46086992502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/348718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae3b4d4ef5d486f96d5ff4dc3399440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a50077871b246bcb20f411229ca553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform_example(example):\n",
    "    history = example['history']\n",
    "    human_ref_A = example['human_ref_A']\n",
    "    human_ref_B = example['human_ref_B']\n",
    "    label = example['labels']\n",
    "\n",
    "    chosen_text = f\"Human: {history} Assistant: {human_ref_A}\" if label == 1 else f\"Human: {history} Assistant: {human_ref_B}\"\n",
    "    rejected_text = f\"Human: {history} Assistant: {human_ref_B}\" if label == 1 else f\"Human: {history} Assistant: {human_ref_A}\"\n",
    "    \n",
    "    return {'chosen': chosen_text, 'rejected': rejected_text}\n",
    "\n",
    "shp_datasets = shp_datasets.map(transform_example, num_proc=16)\n",
    "shp_datasets = shp_datasets.map(preprocess_function, batched=True, num_proc=16)\n",
    "shp_datasets = shp_datasets.filter(\n",
    "        lambda x: len(x[\"input_ids_chosen\"]) <= max_length\n",
    "        and len(x[\"input_ids_rejected\"]) <= max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17791/17791 [48:16<00:00,  6.14it/s] \n"
     ]
    }
   ],
   "source": [
    "shp_prediction_results = np.zeros((len(shp_datasets[\"test\"]),))\n",
    "for i, sample in enumerate(tqdm(shp_datasets[\"test\"])):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=torch.tensor([sample[\"input_ids_chosen\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_chosen\"]]))\n",
    "        sample[\"logits_chosen\"] = output.logits\n",
    "\n",
    "        output = model(input_ids=torch.tensor([sample[\"input_ids_rejected\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_rejected\"]]))\n",
    "        sample[\"logits_rejected\"] = output.logits\n",
    "        \n",
    "        shp_prediction_results[i] = float(sample[\"logits_chosen\"] > sample[\"logits_rejected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7203481263036754}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics((shp_prediction_results, np.ones((len(shp_prediction_results),))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5763929791494876"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prediction_results) / len(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_model = AutoModelForSequenceClassification.from_pretrained(\"reward_modeling_shp\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8489/8489 [18:47<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5289197785369302\n",
      "f1 score {'f1': 0.6918868942137298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions on test set using model\n",
    "shphh_prediction_results = np.zeros((len(raw_datasets[\"test\"]),))\n",
    "for i, sample in enumerate(tqdm(raw_datasets[\"test\"])):\n",
    "    with torch.no_grad():\n",
    "        output = shp_model(input_ids=torch.tensor([sample[\"input_ids_chosen\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_chosen\"]]))\n",
    "        sample[\"logits_chosen\"] = output.logits\n",
    "\n",
    "        output = shp_model(input_ids=torch.tensor([sample[\"input_ids_rejected\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_rejected\"]]))\n",
    "        sample[\"logits_rejected\"] = output.logits\n",
    "        \n",
    "        shphh_prediction_results[i] = float(sample[\"logits_chosen\"] > sample[\"logits_rejected\"])\n",
    "\n",
    "print(f\"accuracy {np.sum(shphh_prediction_results) / len(shphh_prediction_results)}\")\n",
    "print(f\"f1 score {compute_metrics((shphh_prediction_results, np.ones((len(shphh_prediction_results),))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17791/17791 [47:12<00:00,  6.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6979933674329717\n",
      "f1 score {'f1': 0.8221390976199145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shpshp_prediction_results = np.zeros((len(shp_datasets[\"test\"]),))\n",
    "for i, sample in enumerate(tqdm(shp_datasets[\"test\"])):\n",
    "    with torch.no_grad():\n",
    "        output = shp_model(input_ids=torch.tensor([sample[\"input_ids_chosen\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_chosen\"]]))\n",
    "        sample[\"logits_chosen\"] = output.logits\n",
    "\n",
    "        output = shp_model(input_ids=torch.tensor([sample[\"input_ids_rejected\"]]),\n",
    "            attention_mask=torch.tensor([sample[\"attention_mask_rejected\"]]))\n",
    "        sample[\"logits_rejected\"] = output.logits\n",
    "        \n",
    "        shpshp_prediction_results[i] = float(sample[\"logits_chosen\"] > sample[\"logits_rejected\"])\n",
    "\n",
    "print(f\"accuracy {np.sum(shpshp_prediction_results) / len(shpshp_prediction_results)}\")\n",
    "print(f\"f1 score {compute_metrics((shpshp_prediction_results, np.ones((len(shpshp_prediction_results),))))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
